{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing batch gradient without feature scaling\n",
      "[-0.003757210025843233, 0.011335268594940535, -0.0054462433512588605]\n",
      "Accuracy is : 82.75862068965517\n",
      "Doing stochaistic gradient without feature scaling\n",
      "[-17.00941944986729, 0.1507787664831176, 0.16322765871133627]\n",
      "Accuracy is : 89.65517241379311\n",
      "Doing Mini batch gradient without feature scaling\n",
      "[-0.0032175980479680713, 0.008659553033513023, -0.0035662510332845954]\n",
      "Accuracy is : 82.75862068965517\n",
      "Doing batch gradient with feature scaling\n",
      "[0.0007110721689603617, 0.004233063763920973, 0.003164762268784069]\n",
      "Accuracy is : 93.33333333333333\n",
      "Doing stochaistic gradient with feature scaling\n",
      "[0.579981287203592, 7.806737165995212, 6.594950012053912]\n",
      "Accuracy is : 83.33333333333334\n",
      "Doing Mini batch gradient with feature scaling\n",
      "[1.0929605679745887e-06, 0.003365429126505297, 0.002850656019519231]\n",
      "Accuracy is : 86.66666666666667\n"
     ]
    }
   ],
   "source": [
    "# Submitted by Shubham Kumar Bhokta (IIT2020007)\n",
    "\n",
    "\n",
    "# In this code I have implemented logistic regression using batch gradient,\n",
    "#  stochaistic gradient and mini batch gradient for feature scaled and unscaled data.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "def getdata():\n",
    "\tinput_data = pd.read_csv(\"marks.csv\")\n",
    "\tY = input_data['selected']\n",
    "\tmarks1 = input_data['marks1']\n",
    "\tmarks2 = input_data['marks2']\n",
    "\t\n",
    "\tX_train = []\n",
    "\tX_test = []\n",
    "\tY_train = []\n",
    "\tY_test = []\n",
    "\tfor i in range(70):\n",
    "\t\tX_train.append([1, marks1[i], marks2[i]])\n",
    "\t\tY_train.append(Y[i])\n",
    "\n",
    "\tfor i in range(71, 100):\n",
    "\t\tX_test.append([1, marks1[i], marks2[i]])\n",
    "\t\tY_test.append(Y[i])\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def getscaleddata():\n",
    "\tinput_data = pd.read_csv(\"marks.csv\")\n",
    "\tY = input_data['selected']\n",
    "\tmarks1 = input_data['marks1']\n",
    "\tmarks2 = input_data['marks2']\n",
    "\n",
    "\tmeanmarks1 = np.mean(marks1)\n",
    "\tmaxmarks1 = np.max(marks1)\n",
    "\tminmarks1 = np.min(marks1)\n",
    "\n",
    "\tmeanmarks2 = np.mean(marks2)\n",
    "\tmaxmarks2 = np.max(marks2)\n",
    "\tminmarks2 = np.min(marks2)\n",
    "\n",
    "\tX_train = []\n",
    "\tX_test = []\n",
    "\tY_train = []\n",
    "\tY_test = []\n",
    "\n",
    "\tfor i in range(70):\n",
    "\t\tX_train.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2)])\n",
    "\t\tY_train.append(Y[i])\n",
    "\n",
    "\tfor i in range(70, 100):\n",
    "\t\tX_test.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2)])\n",
    "\t\tY_test.append(Y[i])\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + math.exp(-1 * z))\n",
    "\n",
    "# Function to calculate Slope to find coefficients\n",
    "def Slope(Coeff, X_train, Y_train, ind):\n",
    "\tdiff = 0\n",
    "\tfor i in range(len(X_train)):\n",
    "\t\titr = 0\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t\titr = itr + Coeff[j] * X_train[i][j]\n",
    "\t\tdiff += (sigmoid(itr) - Y_train[i]) * X_train[i][ind]\n",
    "\treturn diff\n",
    "\n",
    "# Using batch gradient\n",
    "def batchgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\n",
    "\tCoeff = [0, 0, 0]\n",
    "\tlis1 = []\n",
    "\tfor i in range(epochs):\n",
    "\t\tTempCoeff = Coeff.copy()\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
    "\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "# Finding Accuracy\n",
    "def printaccuracy(X_test, Y_test, Coeff):\n",
    "\tcount = 0\n",
    "\tfor i in range(len(X_test)):\n",
    "\t\tpredicted = 0\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t  \tpredicted = predicted + Coeff[j] * X_test[i][j]\n",
    "\t\tpredicted = sigmoid(predicted)\n",
    "\t\tif predicted > 0.5:\n",
    "\t\t\tif Y_test[i] == 1:\n",
    "\t\t\t\tcount += 1\n",
    "\t\telse:\n",
    "\t\t\tif Y_test[i] == 0:\n",
    "\t\t\t\tcount += 1\n",
    "\tprint(\"Accuracy is : \" + str(count / len(Y_test) * 100))\n",
    "\n",
    "def SlopeStoch(Coeff, X_train, ActualVal, ind):\n",
    "\titr = 0\n",
    "\tfor j in range(len(Coeff)):\n",
    "\t\titr = itr + Coeff[j] * X_train[j]\n",
    "\treturn (sigmoid(itr) - ActualVal) * X_train[ind]\n",
    "\n",
    "def stochgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\tCoeff = [0, 0, 0]\n",
    "\tfor iter in range(epochs):\n",
    "\t\tfor i in range(len(Y_train)):\n",
    "\t\t\tTempCoeff = Coeff.copy()\n",
    "\t\t\tfor j in range(3):\n",
    "\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
    "\t\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "def minibtchgra(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20):\n",
    "\tLearningRateScaling = alpha\n",
    "\tCoeff = [0, 0, 0]\n",
    "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
    "\tequallyDiv = False\n",
    "\tif (len(Y_train) % batchsize == 0):\n",
    "\t\tequallyDiv = True;\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor batch in range(NoOfBatches):\n",
    "\t\t\tSummation = [0, 0, 0]\n",
    "\t\t\tfor j in range(len(Coeff)):\n",
    "\t\t\t\tfor i in range(batchsize):\n",
    "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tPredictedValue = 0.0\n",
    "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
    "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
    "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
    "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
    "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
    "\t\t\t\t\tSummation[j] += PredictedValue;\n",
    "\n",
    "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tCoeff[j] -= (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tCoeff[j] -= (Summation[j] / batchsize) * LearningRateScaling\n",
    "\treturn Coeff\n",
    "\n",
    "# First doing batch gradient, stochaistic gradient and mini batch gradient without feature scaling.\n",
    "X_train, X_test, Y_train, Y_test = getdata()\n",
    "\n",
    "print(\"Doing batch gradient without feature scaling\")\n",
    "coeff = batchgra(X_train, Y_train, 0.00001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing stochaistic gradient without feature scaling\")\n",
    "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing Mini batch gradient without feature scaling\")\n",
    "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "# Now doing batch gradient, stochaistic gradient and mini batch gradient with feature scaling.\n",
    "X_train, X_test, Y_train, Y_test = getscaleddata()\n",
    "\n",
    "print(\"Doing batch gradient with feature scaling\")\n",
    "coeff = batchgra(X_train, Y_train, 0.00001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing stochaistic gradient with feature scaling\")\n",
    "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing Mini batch gradient with feature scaling\")\n",
    "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing batch gradient without feature scaling\n",
      "[-0.09285714285714286, -2.962944402749999, -3.793985451964282, -53.50515248428712, -134.6308304192547, -39.087311554567066, 3813.772544774776, -2518.019699730583, 7435.942238581199, 5810.239816051123]\n",
      "Accuracy is : 80.0\n",
      "Doing stochaistic gradient without feature scaling\n",
      "[-43.43849999998947, -1635.0479041192586, -1835.050026480356, -48394.95114315107, -60847.29545445934, -50599.02472198009, -2169.4011320874347, 1744.0833275123011, 7167.509542840378, -3370.3706969044347]\n",
      "Accuracy is : 86.66666666666667\n",
      "Doing Mini batch gradient without feature scaling\n",
      "[-0.009360000000000005, -0.3662235170338253, -0.38015617379400046, -11.706932864893403, -12.345125492678925, -10.675917365065231, -39.32013516042298, -29.371301130926454, 51.51093223523594, 26.343210377432314]\n",
      "Accuracy is : 70.0\n",
      "Doing batch gradient with feature scaling\n",
      "[0.0007163711061252651, 0.004225909451385743, 0.0031584525799369587, 0.003957192704594542, 0.003145436954465893, 0.004711347610800118, 0.0035719954798885687, 0.0029981642737297374, 0.004245121458872718, 0.004684789864973334]\n",
      "Accuracy is : 86.66666666666667\n",
      "Doing stochaistic gradient with feature scaling\n",
      "[1.5042734546507814, 3.028137779226378, 2.127475481029001, 1.7862959476079467, 1.265642813942837, 4.198883942810742, 0.6018855985199819, 0.4665981638564915, 3.2650812220664833, 3.724583877975815]\n",
      "Accuracy is : 83.33333333333334\n",
      "Doing Mini batch gradient with feature scaling\n",
      "[5.606782415441794e-06, 0.003360601276176679, 0.0028458956292451284, 0.0031411210096455857, 0.0028664655157666567, 0.0038903430569185455, 0.0028288628005613044, 0.0027634201051740352, 0.003435244811929776, 0.003946280853334815]\n",
      "Accuracy is : 73.33333333333333\n"
     ]
    }
   ],
   "source": [
    "# Submitted by Shubham Kumar Bhokta (IIT2020007)\n",
    "\n",
    "# In this code I have implemented logistic regression using batch gradient, stochaistic gradient and mini batch gradient for feature scaled and unscaled data.\n",
    "# Here I have also used higher powers of the data to make more features.\n",
    "# Now the hypothesis looks like h(x) = g(wx) where g(wx) = 1 / (1 + e^(-wx)) and wx = w0 + w1x + w2y + w3x^2 + w4y^2 + w5xy + w6x^3 + w7y^3 + w8x^2y + w9xy^2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "def getdata():\n",
    "\tinput_data = pd.read_csv(\"marks.csv\")\n",
    "\tY = input_data['selected']\n",
    "\tmarks1 = input_data['marks1']\n",
    "\tmarks2 = input_data['marks2']\n",
    "\tmarks1sq = []\n",
    "\tfor i in marks1:\n",
    "\t\tmarks1sq.append(i * i)\n",
    "\n",
    "\tX_train = []\n",
    "\tX_test = []\n",
    "\tY_train = []\n",
    "\tY_test = []\n",
    "\tfor i in range(70):\n",
    "\t\tX_train.append([1, marks1[i], marks2[i], marks1[i] * marks1[i], marks2[i] * marks2[i], marks1[i] * marks2[i], marks1[i] * marks1[i] * marks1[i], marks2[i] * marks2[i] * marks2[i], marks1[i] * marks1[i] * marks2[i], marks1[i] * marks2[i] * marks2[i]])\n",
    "\t\tY_train.append(Y[i])\n",
    "\n",
    "\tfor i in range(70, 100):\n",
    "\t\tX_test.append([1, marks1[i], marks2[i], marks1[i] * marks1[i], marks2[i] * marks2[i], marks1[i] * marks2[i], marks1[i] * marks1[i] * marks1[i], marks2[i] * marks2[i] * marks2[i], marks1[i] * marks1[i] * marks2[i], marks1[i] * marks2[i] * marks2[i]])\n",
    "\t\tY_test.append(Y[i])\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def getscaleddata():\n",
    "\tinput_data = pd.read_csv(\"marks.csv\")\n",
    "\tY = input_data['selected']\n",
    "\tmarks1 = input_data['marks1']\n",
    "\tmarks2 = input_data['marks2']\n",
    "\n",
    "\tmeanmarks1 = np.mean(marks1)\n",
    "\tmaxmarks1 = np.max(marks1)\n",
    "\tminmarks1 = np.min(marks1)\n",
    "\n",
    "\tmeanmarks2 = np.mean(marks2)\n",
    "\tmaxmarks2 = np.max(marks2)\n",
    "\tminmarks2 = np.min(marks2)\n",
    "\n",
    "\tmarks1sq = []\n",
    "\tfor i in marks1:\n",
    "\t\tmarks1sq.append(i * i)\n",
    "\tmeanmarks1sq = np.mean(marks1sq)\n",
    "\tmaxmarks1sq = np.max(marks1sq)\n",
    "\tminmarks1sq = np.min(marks1sq)\n",
    "\tfor i in range(len(marks1sq)):\n",
    "\t\tmarks1sq[i] = (marks1sq[i] - meanmarks1sq) / (maxmarks1sq - minmarks1sq)\n",
    "\n",
    "\tmarks2sq = []\n",
    "\tfor i in marks2:\n",
    "\t\tmarks2sq.append(i * i)\n",
    "\tmeanmarks2sq = np.mean(marks2sq)\n",
    "\tmaxmarks2sq = np.max(marks2sq)\n",
    "\tminmarks2sq = np.min(marks2sq)\n",
    "\tfor i in range(len(marks2sq)):\n",
    "\t\tmarks2sq[i] = (marks2sq[i] - meanmarks2sq) / (maxmarks2sq - minmarks2sq)\n",
    "\n",
    "\tmarks1marks2 = []\n",
    "\tfor i in range(len(marks1)):\n",
    "\t\tmarks1marks2.append(marks1[i] * marks2[i])\n",
    "\tmeanmarks1marks2 = np.mean(marks1marks2)\n",
    "\tmaxmarks1marks2 = np.max(marks1marks2)\n",
    "\tminmarks1marks2 = np.min(marks1marks2)\n",
    "\tfor i in range(len(marks1marks2)):\n",
    "\t\tmarks1marks2[i] = (marks1marks2[i] - meanmarks1marks2) / (maxmarks1marks2 - minmarks1marks2)\n",
    "\n",
    "\tmarks1cu = []\n",
    "\tfor i in marks1:\n",
    "\t\tmarks1cu.append(i * i * i)\n",
    "\tmeanmarks1cu = np.mean(marks1cu)\n",
    "\tmaxmarks1cu = np.max(marks1cu)\n",
    "\tminmarks1cu = np.min(marks1cu)\n",
    "\tfor i in range(len(marks1cu)):\n",
    "\t\tmarks1cu[i] = (marks1cu[i] - meanmarks1cu) / (maxmarks1cu - minmarks1cu)\n",
    "\n",
    "\tmarks2cu = []\n",
    "\tfor i in marks2:\n",
    "\t\tmarks2cu.append(i * i * i)\n",
    "\tmeanmarks2cu = np.mean(marks2cu)\n",
    "\tmaxmarks2cu = np.max(marks2cu)\n",
    "\tminmarks2cu = np.min(marks2cu)\n",
    "\tfor i in range(len(marks2cu)):\n",
    "\t\tmarks2cu[i] = (marks2cu[i] - meanmarks2cu) / (maxmarks2cu - minmarks2cu)\n",
    "\n",
    "\tmarks1sqmarks2 = []\n",
    "\tfor i in range(len(marks1)):\n",
    "\t\tmarks1sqmarks2.append(marks1[i] * marks1[i] * marks2[i])\n",
    "\tmeanmarks1sqmarks2 = np.mean(marks1sqmarks2)\n",
    "\tmaxmarks1sqmarks2 = np.max(marks1sqmarks2)\n",
    "\tminmarks1sqmarks2 = np.min(marks1sqmarks2)\n",
    "\tfor i in range(len(marks1sqmarks2)):\n",
    "\t\tmarks1sqmarks2[i] = (marks1sqmarks2[i] - meanmarks1sqmarks2) / (maxmarks1sqmarks2 - minmarks1sqmarks2)\n",
    "\n",
    "\tmarks2sqmarks1 = []\n",
    "\tfor i in range(len(marks1)):\n",
    "\t\tmarks2sqmarks1.append(marks1[i] * marks2[i] * marks2[i])\n",
    "\tmeanmarks2sqmarks1 = np.mean(marks2sqmarks1)\n",
    "\tmaxmarks2sqmarks1 = np.max(marks2sqmarks1)\n",
    "\tminmarks2sqmarks1 = np.min(marks2sqmarks1)\n",
    "\tfor i in range(len(marks2sqmarks1)):\n",
    "\t\tmarks2sqmarks1[i] = (marks2sqmarks1[i] - meanmarks2sqmarks1) / (maxmarks2sqmarks1 - minmarks2sqmarks1)\n",
    "\n",
    "\tX_train = []\n",
    "\tX_test = []\n",
    "\tY_train = []\n",
    "\tY_test = []\n",
    "\n",
    "\tfor i in range(70):\n",
    "\t\tX_train.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
    "\t\tY_train.append(Y[i])\n",
    "\n",
    "\tfor i in range(70, 100):\n",
    "\t\tX_test.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
    "\t\tY_test.append(Y[i])\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def sigmoid(z):\n",
    "\ttry:\n",
    "\t\tans = 1.0 / (1 + math.exp(-1 * z))\n",
    "\texcept OverflowError:\n",
    "\t\tans = 0\n",
    "\treturn ans\n",
    "\n",
    "# Function to calculate Slope to find coefficients\n",
    "def Slope(Coeff, X_train, Y_train, ind):\n",
    "\tdiff = 0\n",
    "\tfor i in range(len(X_train)):\n",
    "\t\titr = 0\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t\titr = itr + Coeff[j] * X_train[i][j]\n",
    "\t\tdiff += (sigmoid(itr) - Y_train[i]) * X_train[i][ind]\n",
    "\treturn diff\n",
    "\n",
    "# Using batch gradient\n",
    "def batchgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tlis1 = []\n",
    "\tfor i in range(epochs):\n",
    "\t\tTempCoeff = Coeff.copy()\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
    "\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "# Finding Accuracy\n",
    "def printaccuracy(X_test, Y_test, Coeff):\n",
    "\tcount = 0\n",
    "\tfor i in range(len(X_test)):\n",
    "\t\tpredicted = 0\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t  \tpredicted = predicted + Coeff[j] * X_test[i][j]\n",
    "\t\tpredicted = sigmoid(predicted)\n",
    "\t\tif predicted > 0.5:\n",
    "\t\t\tif Y_test[i] == 1:\n",
    "\t\t\t\tcount += 1\n",
    "\t\telse:\n",
    "\t\t\tif Y_test[i] == 0:\n",
    "\t\t\t\tcount += 1\n",
    "\tprint(\"Accuracy is : \" + str(count / len(Y_test) * 100))\n",
    "\n",
    "def SlopeStoch(Coeff, X_train, ActualVal, ind):\n",
    "\titr = 0\n",
    "\tfor j in range(len(Coeff)):\n",
    "\t\titr = itr + Coeff[j] * X_train[j]\n",
    "\treturn (sigmoid(itr) - ActualVal) * X_train[ind]\n",
    "\n",
    "def stochgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tfor iter in range(epochs):\n",
    "\t\tfor i in range(len(Y_train)):\n",
    "\t\t\tTempCoeff = Coeff.copy()\n",
    "\t\t\tfor j in range(len(Coeff)):\n",
    "\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
    "\t\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "def minibtchgra(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20):\n",
    "\tLearningRateScaling = alpha\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
    "\tequallyDiv = False\n",
    "\tif (len(Y_train) % batchsize == 0):\n",
    "\t\tequallyDiv = True;\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor batch in range(NoOfBatches):\n",
    "\t\t\tSummation = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\t\t\tfor j in range(len(Coeff)):\n",
    "\t\t\t\tfor i in range(batchsize):\n",
    "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tPredictedValue = 0.0\n",
    "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
    "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
    "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
    "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
    "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
    "\n",
    "\t\t\t\t\tSummation[j] += PredictedValue;\n",
    "\n",
    "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tCoeff[j] -= (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tCoeff[j] -= (Summation[j] / batchsize) * LearningRateScaling\n",
    "\treturn Coeff\n",
    "\n",
    "# First doing batch gradient, stochaistic gradient and mini batch gradient without feature scaling.\n",
    "X_train, X_test, Y_train, Y_test = getdata()\n",
    "\n",
    "print(\"Doing batch gradient without feature scaling\")\n",
    "coeff = batchgra(X_train, Y_train, 0.1, 5)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing stochaistic gradient without feature scaling\")\n",
    "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing Mini batch gradient without feature scaling\")\n",
    "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "# Now doing batch gradient, stochaistic gradient and mini batch gradient with feature scaling.\n",
    "X_train, X_test, Y_train, Y_test = getscaleddata()\n",
    "\n",
    "print(\"Doing batch gradient with feature scaling\")\n",
    "coeff = batchgra(X_train, Y_train, 0.00001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing stochaistic gradient with feature scaling\")\n",
    "coeff = stochgra(X_train, Y_train, 0.001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing Mini batch gradient with feature scaling\")\n",
    "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 20)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

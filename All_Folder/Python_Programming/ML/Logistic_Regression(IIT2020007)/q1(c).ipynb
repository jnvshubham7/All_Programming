{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing batch gradient without regularisation\n",
      "[0.00014294109269907482, 0.0008466291272877591, 0.0006328899627740144, 0.0007928664267128853, 0.0006303380253779907, 0.0009439414551390919, 0.0007157627554145762, 0.0006008736216772345, 0.0008505735436931656, 0.0009386974687037283]\n",
      "Accuracy is : 86.20689655172413\n",
      "Doing stochaistic gradient without regularisation\n",
      "[0.4719748085008133, 1.266603181118865, 0.8734433801314477, 1.1108427309528415, 0.809090895300768, 1.404869142639804, 0.9277985971527912, 0.7183970059582026, 1.225177638068265, 1.3376390037180164]\n",
      "Accuracy is : 86.20689655172413\n",
      "Doing Mini batch gradient without regularisation\n",
      "[0.0003139158567546346, 0.0021139801895686894, 0.002286468359500325, 0.0020092454445606424, 0.002365904470532104, 0.0027402884892648734, 0.001829010812327897, 0.0023467535840395455, 0.0023607218870855306, 0.002922762541773652]\n",
      "Accuracy is : 75.86206896551724\n",
      "Doing batch gradient with regularisation\n",
      "[0.006863560722146764, 0.005891396036649254, 0.004402230240029623, 0.005515685122947359, 0.004383279873095818, 0.006567957545079797, 0.004977713628862961, 0.004177306468807534, 0.005917273358103763, 0.006530089732814027]\n",
      "Accuracy is : 93.10344827586206\n",
      "Doing stochaistic gradient with regularisation\n",
      "[0.05682116114133792, 7.678638524627715e-06, -3.723372564837103e-05, -1.2273696835885114e-05, -5.5379895981841176e-05, -1.8190458673425044e-05, -2.94485642105931e-05, -6.831851949041436e-05, -2.278389703076626e-05, -4.376380862365612e-05]\n",
      "Accuracy is : 82.75862068965517\n",
      "Doing Mini batch gradient with regularisation\n",
      "[0.0030571610923831446, 0.0021763017930656065, 0.002355427457767837, 0.002068798073992219, 0.0024374097198556367, 0.0028217932834047736, 0.0018834674412088585, 0.002417810801544484, 0.0024308480102274723, 0.003010013113226934]\n",
      "Accuracy is : 93.10344827586206\n"
     ]
    }
   ],
   "source": [
    "# Submitted by Shubham Kumar Bhokta (IIT2020007)\n",
    "\n",
    "# In this code I have implemented feature scaled logistic regression using batch gradient, stochaistic gradient and mini batch gradient with and without regularisation.\n",
    "# Here I have also used higher powers of the data to make more features.\n",
    "# Now the hypothesis looks like h(x) = g(wx) where g(wx) = 1 / (1 + e^(-wx)) and wx = w0 + w1x + w2y + w3x^2 + w4y^2 + w5xy + w6x^3 + w7y^3 + w8x^2y + w9xy^2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "def getscaleddata():\n",
    "\tinput_data = pd.read_csv(\"marks.csv\")\n",
    "\tY = input_data['selected']\n",
    "\tmarks1 = input_data['marks1']\n",
    "\tmarks2 = input_data['marks2']\n",
    "\n",
    "\tmeanmarks1 = np.mean(marks1)\n",
    "\tmaxmarks1 = np.max(marks1)\n",
    "\tminmarks1 = np.min(marks1)\n",
    "\n",
    "\tmeanmarks2 = np.mean(marks2)\n",
    "\tmaxmarks2 = np.max(marks2)\n",
    "\tminmarks2 = np.min(marks2)\n",
    "\n",
    "\tmarks1sq = []\n",
    "\tfor i in marks1:\n",
    "\t\tmarks1sq.append(i * i)\n",
    "\tmeanmarks1sq = np.mean(marks1sq)\n",
    "\tmaxmarks1sq = np.max(marks1sq)\n",
    "\tminmarks1sq = np.min(marks1sq)\n",
    "\tfor i in range(len(marks1sq)):\n",
    "\t\tmarks1sq[i] = (marks1sq[i] - meanmarks1sq) / (maxmarks1sq - minmarks1sq)\n",
    "\n",
    "\tmarks2sq = []\n",
    "\tfor i in marks2:\n",
    "\t\tmarks2sq.append(i * i)\n",
    "\tmeanmarks2sq = np.mean(marks2sq)\n",
    "\tmaxmarks2sq = np.max(marks2sq)\n",
    "\tminmarks2sq = np.min(marks2sq)\n",
    "\tfor i in range(len(marks2sq)):\n",
    "\t\tmarks2sq[i] = (marks2sq[i] - meanmarks2sq) / (maxmarks2sq - minmarks2sq)\n",
    "\n",
    "\tmarks1marks2 = []\n",
    "\tfor i in range(len(marks1)):\n",
    "\t\tmarks1marks2.append(marks1[i] * marks2[i])\n",
    "\tmeanmarks1marks2 = np.mean(marks1marks2)\n",
    "\tmaxmarks1marks2 = np.max(marks1marks2)\n",
    "\tminmarks1marks2 = np.min(marks1marks2)\n",
    "\tfor i in range(len(marks1marks2)):\n",
    "\t\tmarks1marks2[i] = (marks1marks2[i] - meanmarks1marks2) / (maxmarks1marks2 - minmarks1marks2)\n",
    "\n",
    "\tmarks1cu = []\n",
    "\tfor i in marks1:\n",
    "\t\tmarks1cu.append(i * i * i)\n",
    "\tmeanmarks1cu = np.mean(marks1cu)\n",
    "\tmaxmarks1cu = np.max(marks1cu)\n",
    "\tminmarks1cu = np.min(marks1cu)\n",
    "\tfor i in range(len(marks1cu)):\n",
    "\t\tmarks1cu[i] = (marks1cu[i] - meanmarks1cu) / (maxmarks1cu - minmarks1cu)\n",
    "\n",
    "\tmarks2cu = []\n",
    "\tfor i in marks2:\n",
    "\t\tmarks2cu.append(i * i * i)\n",
    "\tmeanmarks2cu = np.mean(marks2cu)\n",
    "\tmaxmarks2cu = np.max(marks2cu)\n",
    "\tminmarks2cu = np.min(marks2cu)\n",
    "\tfor i in range(len(marks2cu)):\n",
    "\t\tmarks2cu[i] = (marks2cu[i] - meanmarks2cu) / (maxmarks2cu - minmarks2cu)\n",
    "\n",
    "\tmarks1sqmarks2 = []\n",
    "\tfor i in range(len(marks1)):\n",
    "\t\tmarks1sqmarks2.append(marks1[i] * marks1[i] * marks2[i])\n",
    "\tmeanmarks1sqmarks2 = np.mean(marks1sqmarks2)\n",
    "\tmaxmarks1sqmarks2 = np.max(marks1sqmarks2)\n",
    "\tminmarks1sqmarks2 = np.min(marks1sqmarks2)\n",
    "\tfor i in range(len(marks1sqmarks2)):\n",
    "\t\tmarks1sqmarks2[i] = (marks1sqmarks2[i] - meanmarks1sqmarks2) / (maxmarks1sqmarks2 - minmarks1sqmarks2)\n",
    "\n",
    "\tmarks2sqmarks1 = []\n",
    "\tfor i in range(len(marks1)):\n",
    "\t\tmarks2sqmarks1.append(marks1[i] * marks2[i] * marks2[i])\n",
    "\tmeanmarks2sqmarks1 = np.mean(marks2sqmarks1)\n",
    "\tmaxmarks2sqmarks1 = np.max(marks2sqmarks1)\n",
    "\tminmarks2sqmarks1 = np.min(marks2sqmarks1)\n",
    "\tfor i in range(len(marks2sqmarks1)):\n",
    "\t\tmarks2sqmarks1[i] = (marks2sqmarks1[i] - meanmarks2sqmarks1) / (maxmarks2sqmarks1 - minmarks2sqmarks1)\n",
    "\n",
    "\tX_train = []\n",
    "\tX_test = []\n",
    "\tY_train = []\n",
    "\tY_test = []\n",
    "\n",
    "\tfor i in range(70):\n",
    "\t\tX_train.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
    "\t\tY_train.append(Y[i])\n",
    "\n",
    "\tfor i in range(71, 100):\n",
    "\t\tX_test.append([1, (marks1[i] - meanmarks1) / (maxmarks1 - minmarks1), (marks2[i] - meanmarks2) / (maxmarks2 - minmarks2), marks1sq[i], marks2sq[i], marks1marks2[i], marks1cu[i], marks2cu[i], marks1sqmarks2[i], marks2sqmarks1[i]])\n",
    "\t\tY_test.append(Y[i])\n",
    "\treturn X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def sigmoid(z):\n",
    "\ttry:\n",
    "\t\tans = 1.0 / (1 + math.exp(-1 * z))\n",
    "\texcept OverflowError:\n",
    "\t\tans = 0\n",
    "\treturn ans\n",
    "\n",
    "# Function to calculate Slope to find coefficients\n",
    "def Slope(Coeff, X_train, Y_train, ind):\n",
    "\tdiff = 0\n",
    "\tfor i in range(len(X_train)):\n",
    "\t\titr = 0\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t\titr = itr + Coeff[j] * X_train[i][j]\n",
    "\t\tdiff += (sigmoid(itr) - Y_train[i]) * X_train[i][ind]\n",
    "\treturn diff\n",
    "\n",
    "# Using batch gradient\n",
    "def batchgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tlis1 = []\n",
    "\tfor i in range(epochs):\n",
    "\t\tTempCoeff = Coeff.copy()\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
    "\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "# Finding Accuracy\n",
    "def printaccuracy(X_test, Y_test, Coeff):\n",
    "\tcount = 0\n",
    "\tfor i in range(len(X_test)):\n",
    "\t\tpredicted = 0\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t  \tpredicted = predicted + Coeff[j] * X_test[i][j]\n",
    "\t\tpredicted = sigmoid(predicted)\n",
    "\t\tif predicted > 0.5:\n",
    "\t\t\tif Y_test[i] == 1:\n",
    "\t\t\t\tcount += 1\n",
    "\t\telse:\n",
    "\t\t\tif Y_test[i] == 0:\n",
    "\t\t\t\tcount += 1\n",
    "\tprint(\"Accuracy is : \" + str(count / len(Y_test) * 100))\n",
    "\n",
    "def SlopeStoch(Coeff, X_train, ActualVal, ind):\n",
    "\titr = 0\n",
    "\tfor j in range(len(Coeff)):\n",
    "\t\titr = itr + Coeff[j] * X_train[j]\n",
    "\treturn (sigmoid(itr) - ActualVal) * X_train[ind]\n",
    "\n",
    "def stochgra(X_train, Y_train, alpha = 0.00001, epochs = 50000):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tfor iter in range(epochs):\n",
    "\t\tfor i in range(len(Y_train)):\n",
    "\t\t\tTempCoeff = Coeff.copy()\n",
    "\t\t\tfor j in range(len(Coeff)):\n",
    "\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
    "\t\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "def minibtchgra(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20):\n",
    "\tLearningRateScaling = alpha\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
    "\tequallyDiv = False\n",
    "\tif (len(Y_train) % batchsize == 0):\n",
    "\t\tequallyDiv = True;\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor batch in range(NoOfBatches):\n",
    "\t\t\tSummation = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\t\t\tfor j in range(len(Coeff)):\n",
    "\t\t\t\tfor i in range(batchsize):\n",
    "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tPredictedValue = 0.0\n",
    "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
    "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
    "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
    "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
    "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
    "\n",
    "\t\t\t\t\tSummation[j] += PredictedValue;\n",
    "\n",
    "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tCoeff[j] -= (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tCoeff[j] -= (Summation[j] / batchsize) * LearningRateScaling\n",
    "\treturn Coeff\n",
    "\n",
    "# Using batch gradient\n",
    "def batchgrareg(X_train, Y_train, alpha = 0.00001, epochs = 50000, lambdaparameter = -49):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tlis1 = []\n",
    "\tfor i in range(epochs):\n",
    "\t\tTempCoeff = Coeff.copy()\n",
    "\t\tfor j in range(len(Coeff)):\n",
    "\t\t\tif j == 0:\n",
    "\t\t\t\tTempCoeff[j] = TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
    "\t\t\telse:\n",
    "\t\t\t\tTempCoeff[j] = (1 - alpha * lambdaparameter / len(X_train)) * TempCoeff[j] - ((LearningRateNoScaling / len(X_train)) * (Slope(Coeff, X_train, Y_train, j)))\n",
    "\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "def stochgrareg(X_train, Y_train, alpha = 0.00001, epochs = 50000, lambdaparameter = 1000):\n",
    "\tLearningRateNoScaling = alpha\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tfor iter in range(epochs):\n",
    "\t\tfor i in range(len(Y_train)):\n",
    "\t\t\tTempCoeff = Coeff.copy()\n",
    "\t\t\tfor j in range(len(Coeff)):\n",
    "\t\t\t\tif j == 0:\n",
    "\t\t\t\t\tTempCoeff[j] = TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tTempCoeff[j] = (1 - alpha * lambdaparameter) * TempCoeff[j] - (LearningRateNoScaling * (SlopeStoch(Coeff, X_train[i], Y_train[i], j)))\n",
    "\t\t\tCoeff = TempCoeff.copy()\n",
    "\treturn Coeff\n",
    "\n",
    "def minibtchgrareg(X_train, Y_train, alpha = 0.000000001, epochs = 30, batchsize = 20, LambdaParameter = 10):\n",
    "\tLearningRateScaling = alpha\n",
    "\tCoeff = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\tNoOfBatches = math.ceil(len(Y_train) / batchsize)\n",
    "\tequallyDiv = False\n",
    "\tif (len(Y_train) % batchsize == 0):\n",
    "\t\tequallyDiv = True;\n",
    "\n",
    "\tfor epoch in range(epochs):\n",
    "\t\tfor batch in range(NoOfBatches):\n",
    "\t\t\tSummation = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\t\t\tfor j in range(len(Coeff)):\n",
    "\t\t\t\tfor i in range(batchsize):\n",
    "\t\t\t\t\tif (batch * batchsize + i == len(X_train)):\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\tPredictedValue = 0.0\n",
    "\t\t\t\t\tfor wj in range(len(Coeff)):\n",
    "\t\t\t\t\t\tPredictedValue += Coeff[wj] * X_train[batch * batchsize + i][wj]\n",
    "\t\t\t\t\tPredictedValue = sigmoid(PredictedValue)\n",
    "\t\t\t\t\tPredictedValue -= Y_train[batch * batchsize + i]\n",
    "\t\t\t\t\tPredictedValue *= X_train[batch * batchsize + i][j]\n",
    "\t\t\t\t\tSummation[j] += PredictedValue;\n",
    "\n",
    "\t\t\tif (not equallyDiv and batch == NoOfBatches - 1):\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tif j == 0:\n",
    "\t\t\t\t\t\tCoeff[j] = Coeff[j] - (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tCoeff[j] = (1 - LearningRateScaling * LambdaParameter / (len(Y_test) % batchsize)) * Coeff[j] - (Summation[j] / (len(Y_train) % batchsize)) * LearningRateScaling\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor j in range(len(Summation)):\n",
    "\t\t\t\t\tif j == 0:\n",
    "\t\t\t\t\t\tCoeff[j] = Coeff[j] - (Summation[j] / batchsize) * LearningRateScaling\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tCoeff[j] = (1 - LearningRateScaling * LambdaParameter / batchsize) * Coeff[j] - (Summation[j] / batchsize) * LearningRateScaling\n",
    "\treturn Coeff\n",
    "\n",
    "# First doing batch gradient, stochaistic gradient and mini batch gradient without regularisation.\n",
    "X_train, X_test, Y_train, Y_test = getscaleddata()\n",
    "\n",
    "print(\"Doing batch gradient without regularisation\")\n",
    "coeff = batchgra(X_train, Y_train, 0.00001, 1000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing stochaistic gradient without regularisation\")\n",
    "coeff = stochgra(X_train, Y_train, 0.0001, 5000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing Mini batch gradient without regularisation\")\n",
    "coeff = minibtchgra(X_train, Y_train, 0.0001, 100, 32)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "# Now doing batch gradient, stochaistic gradient and mini batch gradient with regularisation.\n",
    "print(\"Doing batch gradient with regularisation\")\n",
    "coeff = batchgrareg(X_train, Y_train, 0.0001, 5000, 1000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing stochaistic gradient with regularisation\")\n",
    "coeff = stochgrareg(X_train, Y_train, 0.001, 500, 1000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)\n",
    "\n",
    "print(\"Doing Mini batch gradient with regularisation\")\n",
    "coeff = minibtchgrareg(X_train, Y_train, 0.0001, 1000, 32, 1000)\n",
    "print(coeff)\n",
    "printaccuracy(X_test, Y_test, coeff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
